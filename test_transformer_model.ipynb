{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (4.66.5)\n",
      "Requirement already satisfied: decord in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (0.6.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (0.12.1)\n",
      "Collecting lazy-loader\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: resampy in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from lazy-loader) (24.1)\n",
      "Requirement already satisfied: numba>=0.53 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from resampy) (0.61.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from numba>=0.53->resampy) (0.44.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexey\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: lazy-loader\n",
      "Successfully installed lazy-loader-0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\program files\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchaudio torchvision tqdm decord soundfile lazy-loader resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexey\\IU\\Spring 2025\\GAI\\preprocess.py:20: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "C:\\Users\\Alexey\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Alexey\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\Alexey/.cache\\torch\\hub\\harritaylor_torchvggish_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Building vocabulary from: data/v2424877187.irc\n",
      "Vocabulary size: 5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexey\\IU\\Spring 2025\\GAI\\train.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#   MULTIMODAL TRANSFORMER NOTEBOOK\n",
    "# ================================================================\n",
    "# This notebook shows how to:\n",
    "# 1) Build a vocabulary from a chat file\n",
    "# 2) Precompute embeddings for video (via ResNet) and audio (via a placeholder or pretrained model)\n",
    "# 3) Save them in .pt files, one per chat line\n",
    "# 4) Create a dataset + dataloader\n",
    "# 5) Train a Transformer-based multimodal model\n",
    "# 6) Run inference (generate comments) with top-k sampling\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We'll import from our local modules:\n",
    "from data_utils import (\n",
    "    build_vocabulary,\n",
    "    TwitchCommentDataset,\n",
    "    my_collate_fn\n",
    ")\n",
    "from preprocess import preprocess_files\n",
    "from model import MultiModal\n",
    "from train import train_one_epoch\n",
    "from inference import generate_comment\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "CHAT_FILE = \"data/v2424877187.irc\"  # The .irc file with chat lines\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "VIDEO_FILE = \"data/2424877187.mkv\"\n",
    "AUDIO_FILE = \"data/2424877187.wav\"\n",
    "OUTPUT_DIR = \"precomputed_data\"\n",
    "MODEL_PATH = \"multimodal_transformer.pth\"\n",
    "\n",
    "# ==============================\n",
    "# 1) BUILD VOCAB\n",
    "# ==============================\n",
    "\n",
    "print(\"Building vocabulary from:\", CHAT_FILE)\n",
    "word2idx, idx2word = build_vocabulary(\n",
    "    chat_file=CHAT_FILE,\n",
    "    min_freq=1,\n",
    "    max_size=5000,           # Or any limit\n",
    "    special_tokens=special_tokens\n",
    ")\n",
    "vocab_size = len(word2idx)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 2) PREPROCESS & SAVE EMBEDDINGS\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ==============================\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(OUTPUT_DIR):\n\u001b[0;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(OUTPUT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPreprocessing and embedding video/audio => storing in\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_DIR)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 2) PREPROCESS & SAVE EMBEDDINGS\n",
    "# ==============================\n",
    "\n",
    "\n",
    "if not os.path.isdir(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"\\nPreprocessing and embedding video/audio => storing in\", OUTPUT_DIR)\n",
    "preprocess_files(\n",
    "    video_path=VIDEO_FILE,\n",
    "    audio_path=AUDIO_FILE,\n",
    "    chat_file=CHAT_FILE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    snippet_duration=10.0,  # 10 seconds before each chat line\n",
    "    sample_rate=16000,      # For audio\n",
    "    fps_for_sampling=1.0,                # 1 frame per second for video\n",
    "    word2idx=word2idx\n",
    ")\n",
    "print(\"Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexey\\IU\\Spring 2025\\GAI\\data_utils.py:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sample_dict = torch.load(out_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating dataset and dataloader ...\n",
      "Dataset size: 13923\n",
      "Sample batch shapes:\n",
      " video: torch.Size([4, 1, 2048])\n",
      " audio: torch.Size([4, 1, 128])\n",
      " text:  torch.Size([4, 7])\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 3) CREATE DATASET/DATALOADER\n",
    "# ==============================\n",
    "print(\"\\nCreating dataset and dataloader ...\")\n",
    "\n",
    "dataset = TwitchCommentDataset(\n",
    "    cache_dir=OUTPUT_DIR,\n",
    "    chat_file=CHAT_FILE,\n",
    "    word2idx=word2idx\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,        # adjust as needed\n",
    "    shuffle=True,\n",
    "    collate_fn=my_collate_fn\n",
    ")\n",
    "\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(\"Sample batch shapes:\")\n",
    "print(\" video:\", sample_batch['video'].shape)\n",
    "print(\" audio:\", sample_batch['audio'].shape)\n",
    "print(\" text: \", sample_batch['text'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Transformer model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db3cde102ac481c950a6a0711c66351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 10.1997\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b3e074edbd4bf1b6f7a5357f47fea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 4.6357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48299f691a9542dfa7c675f92b3e1fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 2.5302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0e5648e4c44ed09faf70f4031a38bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 1.3396\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be1a73e864f43208b861e3b47473466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 0.5157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38155ee73782430e980456378f373470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 0.1770\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b44a06e3d864ff3a98987dcd4b1a22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Loss: 0.0950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0d7abbf2a74ef088765fc4cde0e74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Loss: 0.0607\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34a448250884edd95b50cdb27e3c499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Loss: 0.0321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa34426691d94b9e9de1a833834ee6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/3481 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Loss: 0.0353\n",
      "\n",
      "Model saved to: multimodal_transformer.pth\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 4) INIT MODEL\n",
    "# ==============================\n",
    "print(\"\\nInitializing Transformer model ...\")\n",
    "\n",
    "# If your preprocess_files used single embeddings => (2048,) for video, (128,) for audio\n",
    "# Then your video_feature_dim=2048, audio_feature_dim=128\n",
    "model = MultiModal(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    video_feature_dim=2048,\n",
    "    audio_feature_dim=128,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# ==============================\n",
    "# 5) TRAIN LOOP\n",
    "# ==============================\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<PAD>\"])\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = train_one_epoch(model, dataloader, optimizer, criterion, epoch, device)\n",
    "    print(f\"[Epoch {epoch}] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(\"\\nModel saved to:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running sample inference ...\n",
      "Original comment: <SOS> ww <EOS>\n",
      "Original comment (tokens): tensor([ 2, 10,  3])\n",
      "Generated comment: @mer01337 взаимно ало? с аганом ))) делал смотрят <EOS>\n",
      "Generated comment (tokens): [108, 4942, 4099, 30, 1920, 164, 1403, 1174, 3]\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# 6) INFERENCE\n",
    "# ==============================\n",
    "print(\"\\nRunning sample inference ...\", end='\\n')\n",
    "rev_vocab = {v: k for k, v in word2idx.items()}\n",
    "\n",
    "# We'll pick an entry from the dataset\n",
    "SAMPLE_IDX = 5000  # or any\n",
    "sample_data = dataset[SAMPLE_IDX]\n",
    "video_emb = sample_data['video']  # (2048,) or (T, 2048)\n",
    "audio_emb = sample_data['audio']  # (128,) or (A_time, 128)\n",
    "\n",
    "orig_text = [rev_vocab.get(tid, \"<UNK>\") for tid in sample_data['text'].cpu().numpy()]\n",
    "print(\"Original comment:\", \" \".join(orig_text))  # Original text\n",
    "print(f\"Original comment (tokens): {sample_data['text']}\", end='\\n')\n",
    "\n",
    "start_tok = word2idx.get(\"<SOS>\", 0)\n",
    "end_tok   = word2idx.get(\"<EOS>\", 0)\n",
    "\n",
    "model = MultiModal(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    video_feature_dim=2048,\n",
    "    audio_feature_dim=128,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('multimodal_transformer_alpha_1.pth', weights_only=True))\n",
    "\n",
    "gen_tokens = generate_comment(\n",
    "    model=model,\n",
    "    video_tensor=video_emb,\n",
    "    audio_tensor=audio_emb,\n",
    "    start_token_idx=start_tok,\n",
    "    end_token_idx=end_tok,\n",
    "    max_len=20,\n",
    "    device=device,\n",
    "    temperature=0.8,\n",
    "    top_k=5\n",
    ")\n",
    "# Convert token IDs => text\n",
    "decoded = [rev_vocab.get(tid, \"<UNK>\") for tid in gen_tokens]\n",
    "print(\"Generated comment:\", \" \".join(decoded))\n",
    "print(f\"Generated comment (tokens): {gen_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
